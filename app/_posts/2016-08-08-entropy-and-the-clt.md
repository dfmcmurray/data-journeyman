---
layout: post
title: "Entropy and the Central Limit Theorem"
blurb: "The Central Limit Theorem is extremely fundamental to statistics, but it's so fundamental that it pops up in other places, like physics, too."
thumbnail: inf-stats-logo.png
tags: 
  - Statistics
---

I've read through various explanations of entropy before, but I've mostly only understood the concept in the domain of information theory, where entropy is a measure of how dense an information representation is. When it comes to the physics concept, I could only regurgitate basic statements like "The entropy of the universe always increases" without much deep understanding.

I recently came across [Aatish Bhatia's great explanation of entropy](https://aatishb.github.io/entropy/), and all that changed. Not only did the article keep my attention with a nice humorous touch, it did a great job of explaining entropy in a physical sense. What struck me the most is how closely the concept of entropy in physics is related to the [Central Limit Theorem](http://www.datajourneyman.com/2015/01/26/Basic-Continuous-Distributions.html#the-central-limit-theorem) in statistics. I highly recommend reading Bhatia's article for an accessible explanation of the physics definition of entropy.
