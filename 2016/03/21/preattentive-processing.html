<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Preattentive Processing</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css><link rel=stylesheet href=/css/main.8ea3.css></head><body><div class=site><div id=header><a href="/"><div class=title></div><!-- <img class="title" src="/img/title-anglecia.0053.png" /> --><!-- <div class='title'>
			Data Journeyman
		</div>	 --><!-- <div class='tagline'>
			One man's journey from noise to knowledge<img src="/img/datum.19d4.png" />
		</div> --></a></div><div class=post-container><div class=post-heading><h2>Preattentive Processing</h2><p class=meta>21 Mar 2016</p><div class=tags><span class="tag Visualization">Visualization</span></div></div><div class=post><h3 id=visualization-recap>Visualization Recap</h3><p>Before getting into the crux of this post, I’d like to take stock of what we’ve covered in this series of posts on Data Visualization so far, and how the information in those posts relates to the topic of this one: <a href=https://en.wikipedia.org/wiki/Pre-attentive_processing>preattentive processing</a>.</p><h4 id=post-1-data-visualizationhttpwwwdatajourneymancom20160229data-visualizationhtml>Post 1: <a href=http://www.datajourneyman.com/2016/02/29/data-visualization.html>Data Visualization</a></h4><p>In the first post on visualization, I showed how the visual system dominates over the other human senses in terms of bandwidth. This extraordinary processing power is what enables preattentive processing. Since the brain’s ability to process visual information is so great, much of it happens outside of the slower mechanism of attention.</p><h4 id=post-2-visual-encodingshttpwwwdatajourneymancom20160307visual-encodingshtml>Post 2: <a href=http://www.datajourneyman.com/2016/03/07/visual-encodings.html>Visual Encodings</a></h4><p>In the second post on visualization, the paper I cited by Cleveland and McGill tests each visual encoding’s effectiveness by measuring preattentive processing, or as the authors put it, the “instantaneous perception of the visual field that comes without apparent mental effort.” This stage of preattentive processing affected the overall accuracy of the viewer when judging value changes across various visual encodings.</p><h4 id=post-3-colorhttpwwwdatajourneymancom20160314colorhtml>Post 3: <a href=http://www.datajourneyman.com/2016/03/14/color.html>Color</a></h4><p>Color, with its ability to highlight pertinent information, is an excellent tool for triggering preattentive processing of salient data. In <a href=https://www.perceptualedge.com/articles/b-eye/choosing_colors.pdf>the words of Maureen Stone</a>, “Contrasting colors are different, analogous colors are similar. Contrast draws attention, analogy groups.” As we’ll see in this post, this effect of grouping and separating items based on color happens at the preattentive stage.</p><h3 id=preattentive-processing>Preattentive Processing</h3><p>To take advantage of the huge bandwidth of the visual system, you must convey information to the viewer at the preattentive level because preattentive processing is parallel, whereas attentive processing is serial. It is actually now accespted that attention plays a critical role in the entire visual process, but this terminology is still used since it’s intuitive that attention as we think of it is a slower process.</p><h1 class=pull-quote>The human brain is able to spot the salient information in a fraction of a second</h1><p>Stephen Few goes into this visual process in his article <a href=http://www.perceptualedge.com/articles/ie/visual_perception.pdf>Tapping the Power of Visual Perception</a>. Take the following example of his to illustrate the power of preattentive processing. Assume the most salient information in this visual is the number of fives present. In the top box, all of the numbers are the same color, and no preattentive processing occurs. In the second box, the fives have a different color value, and are therefore trivially easy for the viewer to pick out and count.</p><p><img alt="Preattentive Processing Example" src=/img/preattentive-fives.cf9a.png class=full-size></p><p>It’s important to note that preattentive processing is only effective if it’s used to emphasize the salient information. If the most important thing for the viewer to see is the number of sixes in the visualization, then there is no advantage from preattentive processing in either box.</p><p>The following diagram (adapted from Alan D. Baddeley’s <a href=http://media.johnwiley.com.au/product_data/excerpt/1X/04700914/047009141X.pdf>The Psychology of Memory</a>) demonstrates where preattentive processing occurs on the memory pipeline. Note that this diagram is very much a simplification.</p><p><img alt="Memory Model: Preattentive Attention, Short Term Memory, Long Term Memory" src=/img/memory-model.9cb3.png class=full-size></p><p>Taking this memory model into consideration, short term memory is really the bottleneck of the whole process, since it can only hold 5 to 9 chunks of information at any given time. We can reduce this problem by allowing preattentive processing to handle some of the information load by chunking the data in a meaningful way. Furthermore, animation and interaction can allow emphasis to be given to different parts of the data at different times, allowing the viewer to take in the information at a more manageable rate.</p><p>To illustrate just how fast preattentive processing is, you can watch <a href="https://www.youtube.com/watch?v=wnvoZxe95bo&amp;ebc=ANyPxKoPKNVc2XI1vQuAwQxnL9TejWaWuwLxVU0axqAG2OyUL9sPfPO5c-TimZiLh1SLnIzKcLlM">this three minute video</a>. The human brain is able to spot the salient information (the anomaly in the data) in a fraction of a second. If you have a lot of data to present, using methods that utilize the viewer’s preattentive processing capabilities can steer her to the most important data to remember first, since after making around 7 observations, the short term memory capacity will have been met.</p><h3 id=methods-for-activating-preattentive-processing>Methods for Activating Preattentive Processing</h3><p>A <a href=http://www.csc.ncsu.edu/faculty/healey/download/tvcg.12a.pdf>2012 IEEE paper by Healy and Enns</a> describes preattentive processing as “the way human vision rapidly and automatically categorizes visual images into regions and properties based on simple computations that can be made in parallel across an image.” They cover the human visual system and the various theories around preattentive processing very thoroughly, so if you’re interested in going further into this topic, I highly suggest you read their paper.</p><p>Not every visual encoding can be used to trigger preattentive processing. In fact, sometimes a particular encoding is effective only in certain cases. Healy and Enns reference a work by A. Treisman that showed “a sloped line in a sea of vertical lines can be detected preattentively, but a vertical line in a sea of sloped lines cannot.” Here is a list of features that can trigger preattentive processing (defined as “tasks that can be performed on large multi-element displays in less than 200-250 milliseconds”).</p><p><img alt="Examples of Preattentive Features" src=/img/preattentive-features.3a60.png class=full-size></p><h4 id=similarity-theory>Similarity Theory</h4><h1 class="pull-quote right">there has to be some background data for anything to appear in the foreground</h1><p>You might notice that there’s a common component in each of the preattentive feature visualizations above that allows certain data elements to pop out: a sort of background of uniform objects. That requirement is accounted for in Duncan and Humphreys’ Similarity Theory. In this theory, the search time for “target” elements depends on two measures: T-N Similarity and N-N Similarity. T-N Similarity is the amount of similar features shared by targets and nontargets, and as it goes up, it’s harder to detect the target. N-N Similarity is the amount of similarity within the nontargets themselves, and as it goes up, it’s easier to detect the target.</p><p><img src=/img/preattentive-example.19e1.png alt="Preattentive Processing Example of Confounding Factors"></p><p>In the above figure, the task is to determine which images have a red circle in each pairing (a-b, c-d, e-f). The a-b and c-d pairs are easy to figure out quickly because all of the nontarget elements are identical. However, when the two nontarget elements are combined for e-f, N-N Similarity is reduced, and therefore it’s harder to detect the red circle.</p><p>Similarity Theory effect is closely related to the <a href=http://graphicdesign.spokanefalls.edu/tutorials/process/gestaltprinciples/gestaltprinc.htm>principals of Gestalt</a>. These principals are useful if you want to group certain data elements together, or make data elements stand out. Remember, when emphasizing data, there has to be some background data for anything to appear in the foreground.</p><h4 id=ranking-preattentive-features>Ranking Preattentive Features</h4><p>As for whether certain preattentive features are more effective than others, the answer is yes, but it’s a complicated yes. Healy and Enns refer to this ordering as “rough”, and many of these features in the ranking, such as 3D layout, are not even that appropriate for the typical visualization of abstract information. The ranking goes as follows.</p><ol><li>Determine the 3D layout of a scene;</li><li>Determine surface structures and volumes;</li><li>Establish object movement;</li><li>Interpret luminance gradients across surfaces; and</li><li>Use color to fine tune these interpretations.</li></ol><p>Typically, when any of these features interfere with each other, it’s the feature earlier on the list that takes precedence. Again though, this list is hardly an excuse to start using 3D embellishments on your graphs, and preattentive features in visualizations should be designed not to conflict anyway. After all, data visualization is a medium that is under much more control than the visual scenes our brains evolved to handle, and that should be used as an advantage.</p><h3 id=further-research-on-the-visual-system>Further Research on the Visual System</h3><p>There is plenty of additional research on this topic, as well. Similarity Theory hypothesizes about bottom-up processes that don’t account for other top-down processes, like someone with a particular goal in mind would have. This goal could be internal or external. In the case of the Similarity Theory example above, the participant has an external goal since she is told to look for a red circle, and this directive influences how the participant perceives the figure. More advanced theories account for top-down processes as well, like <a href=https://en.wikipedia.org/wiki/Visual_search#Guided_search_model>Guided Search Theory</a> by Wolfe et al.</p><p>Beyond that, researchers have even found that more information can be taken in during the preattentive stage than just singular attributes. For instance, D. Ariely showed that “observers could extract the average size of a large number of dots from a single glimpse of a display. Yet, when observers were tested on the same displays and asked to indicate whether a specific dot of a given size was present, they were unable to do so. This suggests that there is a preattentive mechanism that records summary statistics of visual features without retaining information about the constituent elements that generated the summary.”</p><p>Knowing the tendencies and capabilities of human perception and the brain is crucial for a data visualizer. Remember what I said in a previous post, that “you must keep your audience in mind at all times to arrive at the most effective visualization possible.” This advice applies to every aspect of your audience, even the parts that they themselves aren’t aware of.</p></div><div class=comments><div id=disqus_thread></div><script type=text/javascript>/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
	        var disqus_shortname = 'datajourneyman'; // required: replace example with your forum shortname

	        /* * * DON'T EDIT BELOW THIS LINE * * */
	        (function() {
	            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	        })();</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=http://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><script src=//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js></script><script src=//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js></script><script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({
		"HTML-CSS": { linebreaks: { automatic: true } },
		SVG: { linebreaks: { automatic: true } },
		TeX: { 
            Macros: { 
              goodbreak: '\\mmlToken{mo}[linebreak="goodbreak"]{}', 
              badbreak: ['\\mmlToken{mo}[linebreak="badbreak"]{#1}',1], 
              nobreak: ['\\mmlToken{mo}[linebreak="nobreak"]{#1}',1], 
              invisibletimes: ['\\mmlToken{mo}{\u2062}'] 
            } 
        } 
    });</script><script src=/js/scripts.08ae.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-10466691-5', 'auto');
  ga('send', 'pageview');</script></body></html>