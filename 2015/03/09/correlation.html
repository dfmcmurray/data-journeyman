<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Correlation</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css><link rel=stylesheet href=/css/main.6763.css></head><body><div class=site><div id=header><a href="/"><div class=title></div><!-- <img class="title" src="/img/title-anglecia.0948.png" /> --><!-- <div class='title'>
			Data Journeyman
		</div>	 --><!-- <div class='tagline'>
			One man's journey from noise to knowledge<img src="/img/datum.e49d.png" />
		</div> --></a></div><div class=post-container><div class=post-heading><h2>Correlation</h2><p class=meta>09 Mar 2015</p><div class=tags><span class="tag Curriculum">Curriculum</span> <span class="tag Think Stats">Think Stats</span> <span class="tag Statistics">Statistics</span> <span class="tag Book">Book</span> <span class="tag Tools and Resources">Tools and Resources</span></div></div><div class=post><h3 id=preface-another-worthwhile-blog>Preface: Another Worthwhile Blog</h3><p>Before we dig into the material of the final chapter of <em>Think Stats</em>, I want to point out a great new blog on probability, <a href="http://www.countbayesie.com/">Count Baysie</a>. It’s written by the lead data scientist of the analytics company <a href="https://www.kissmetrics.com/">KISSmetrics</a>. As luck would have it, his latest post is titled <a href=http://www.countbayesie.com/blog/2015/2/21/variance-co-variance-and-correlation>Understanding Variance, Co-Variance, and Correlation</a>, and due to its clarity on the topic, I’ll be weaving in several of his explanations with the structure of <em>Think Stat</em>’s take on the topic to maximize the intuition around the idea of correlation.</p><h3 id=variance-revisited>Variance Revisited</h3><p>Recall from <a href=http://www.datajourneyman.com/2014/11/06/probability-and-statistics-terms.html>a previous post</a> that the definition of variance is</p><p>\[Var(X) = \sum_{\forall i} (X_i - \mu)^2 \times P(X_i) = E[(X - \mu)^2]\]</p><p>Another way to write variance, since \(\mu = E(X)\), would be</p><p>\[ \begin{eqnarray} Var(X) &amp;=&amp; E[(X - E[X])^2]\cr &amp;=&amp; E[X^2 - 2X E[X] + (E[X])^2]\cr &amp;=&amp; E[X^2] - 2 E[X] E[X] + (E[X])^2\cr &amp;=&amp; E[X^2] - 2 (E[X])^2 + (E[X])^2\cr &amp;=&amp; E[X^2] - (E[X])^2 \end{eqnarray} \]</p><p>which is also called “mean of square minus square of mean” as a way to remember it, and it’s a great way to think about covariance.</p><h3 id=covariance>Covariance</h3><p>Covariance is a measure of how two variables correspond to one another.</p><p>If we take where we ended up with variance above, and write out the exponents as multiplication, we can replace one of the \(X\) random variables with a different random variable \(Y\) to get a measure of covariance between the two random variables.</p><p>\[ \begin{eqnarray} Var(X) &amp;=&amp; E[X X] - E[X] E[X]\cr Cov(X, Y) &amp;=&amp; E[XY] - E[X]E[Y] \end{eqnarray} \]</p><p>In other words, variance can be calculated by plugging in the same random variable for both parameters of the covariance function (i.e., \(Cov(X, X)\)).</p><h4 id=problems-with-covariance>Problems with Covariance</h4><p>One problem with covariance is the units it’s in, namely the units of X times the units of Y.</p><p>Another problem with covariance is that it’s hard to interpret because its magnitude is subject to the respective variances of the original distributions. For distributions \(X’ = cX\) and \(Y’ = dY\), where c and d are constants,</p><p>\[ \begin{eqnarray} Cov(X’, Y’) &amp;=&amp; Cov(cX, dY)\cr &amp;=&amp; E[cXdY] - E[cX]E[dY]\cr &amp;=&amp; c \cdot d \cdot E[XY] - c \cdot d \cdot E[X]E[Y]\cr &amp;=&amp; c \cdot d \cdot (E[XY] - E[X]E[Y])\cr &amp;=&amp; c \cdot d \cdot Cov(X, Y) \end{eqnarray} \]</p><p>So, this is definitely a problem, because what we would really like is a measure that tells us how much we know about one variable if we know the value of the other, regardless of the variation within each distribution and between them.</p><h3 id=correlation>Correlation</h3><p>We can fix both the dimension problem and the scaling problem with one change to the covariance function, and this resulting value will be correlation.</p><p>\[Corr(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}}\]</p><p>Now we have a dimensionless measure for the relationship between X and Y. Additionally, correlation is always between -1 and 1. A correlation of 1 means that the if you know one of the variables, you can perfectly predict the other. A correlation of -1 means you can make perfect predictions as well; it’s just that the variables are moving in opposite directions from each other. A correlation of 0 means that no predicioon can be made. And anything in between just means that an imperfect prediction is the best you can do, getting better as the magnitude of the correlation approaches 1.</p><p><img src=/img/correlations.4d4d.png alt="Examples correlations"></p><p>There are several things to note about the figure above. First is that the “fuzzier” the data, the closer the correlation is to 0. Second is that some of the datasets have nonlinear relationships that correlation doesn’t pick up at all. Third is that, of the datasets with a correlation of 1 or -1, the correlation doesn’t tell you anything about the slope of the line. This third point can be addressed with Simple Linear Regression, or the Lease Squares Fit.</p><h3 id=least-squares-fit>Least Squares Fit</h3><p>The idea of linear regression is to pick a line (\(y_i = \alpha + \beta x_i\)) that best matches your data. Then, if you have a new x value, you can approximate the corresponding y value. But unless your data has a correlation of 1 (or -1), then there is no perfect line that you can draw to capture your data. That is, there will necessarily be some error, \(\epsilon_i = \alpha + \beta x_i - y_i\).</p><p>A linear regression technique will minimize this error. Least Squares Fit, like the name suggests, minimizes the square of the errors.</p><p>\[\min_{\alpha, \beta} \sum \epsilon^2_i\]</p><p>There are a number of good reasons for choosing Least Squares Fit.</p><ol><li>Squaring treats positive and negative errors the same.</li><li>Squaring gives larger errors more weight, so in minimizing we correct for greater errors more.</li><li>Under certain conditions (errors are independent of x, random, and normally distributed), then the Least Squares Fit estimators for \(\alpha\) and \(\beta\) are their Maximum Likelihood Estimators.</li></ol><p>For Least Squares Fit, the estimators for \(\alpha\) and \(\beta\) are</p><p>\[\hat \alpha = \bar y - \hat \beta \bar x\] \[\hat \beta = \frac{Cov(X, Y)}{Var(X)}\]</p><h3 id=conclusion>Conclusion</h3><p>We will undoubtably use Least Squares Fit and the related concepts in many future applications. In fact, <em>Think Stats</em> has provided us with an amazing jumping off point into the world of data, which must be rooted in the world of probability and statistics.</p></div><div class=comments><div id=disqus_thread></div><script type=text/javascript>/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
	        var disqus_shortname = 'datajourneyman'; // required: replace example with your forum shortname

	        /* * * DON'T EDIT BELOW THIS LINE * * */
	        (function() {
	            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	        })();</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=http://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><script src=//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js></script><script src=//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js></script><script src=//cdn.blockspring.com/blockspring.js></script><script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({
		"HTML-CSS": { linebreaks: { automatic: true } },
		SVG: { linebreaks: { automatic: true } },
		TeX: { 
            Macros: { 
              goodbreak: '\\mmlToken{mo}[linebreak="goodbreak"]{}', 
              badbreak: ['\\mmlToken{mo}[linebreak="badbreak"]{#1}',1], 
              nobreak: ['\\mmlToken{mo}[linebreak="nobreak"]{#1}',1], 
              invisibletimes: ['\\mmlToken{mo}{\u2062}'] 
            } 
        } 
    });</script><script src=/js/scripts.08ae.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-10466691-5', 'auto');
  ga('send', 'pageview');</script></body></html>