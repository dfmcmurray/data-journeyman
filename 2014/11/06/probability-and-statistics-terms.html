<!DOCTYPE html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Probability and Statistics Terms</title><meta name=viewport content="width=device-width"><link rel=stylesheet href=//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css><link rel=stylesheet href=/css/main.8ea3.css></head><body><div class=site><div id=header><a href="/"><div class=title></div><!-- <img class="title" src="/img/title-anglecia.0053.png" /> --><!-- <div class='title'>
			Data Journeyman
		</div>	 --><!-- <div class='tagline'>
			One man's journey from noise to knowledge<img src="/img/datum.19d4.png" />
		</div> --></a></div><div class=post-container><div class=post-heading><h2>Probability and Statistics Terms</h2><p class=meta>06 Nov 2014</p><div class=tags><span class="tag Curriculum">Curriculum</span> <span class="tag Think Stats">Think Stats</span> <span class="tag Statistics">Statistics</span> <span class="tag Book">Book</span> <span class="tag Terms">Terms</span> <span class="tag IPython">IPython</span></div></div><div class=post><h3 id=initial-probability-and-statistics-concepts>Initial Probability and Statistics Concepts</h3><p>In this post, we’ll take a look at some terms that you probably already know, mixed with some that may be a little more foreign (or maybe just more forgotten). However, I’m going to flip the order in which the material is traditionally presented (<em>Think Stats</em> follows the traditional order). This flipped ordering will allow us to look at some of the most basic concepts (like mean and variance) from a wholistic point of view, using the slightly more advanced concept of Probability Mass Functions.</p><h3 id=data-representations>Data Representations</h3><h4 id=histogram>Histogram</h4><p>The most ubiquitous data representation around is the histogram. It’s a representation of how a data set is distributed. A histogram consists of a set of bins, and for each bin an integer representing the number of values falling within that bin.</p><p>Let’s take the following data as an example.</p><p>\[ \begin{array}29, 65, 89, 74, 54, 33, 11, 52, 33, 81, 75, 85, 53, 81, 59, 13, 96, 24, 16, 58\end{array} \]</p><p>A histogram representation would look like this:</p><p>\[ \begin{eqnarray} 0-9 &amp;:&amp; 0\cr 10-19 &amp;:&amp; 3\cr 20-29 &amp;:&amp; 2\cr 30-39 &amp;:&amp; 2\cr 40-49 &amp;:&amp; 0\cr 50-59 &amp;:&amp; 5\cr 60-69 &amp;:&amp; 1\cr 70-79 &amp;:&amp; 2\cr 80-89 &amp;:&amp; 4\cr 90-99 &amp;:&amp; 1\cr \end{eqnarray} \]</p><p>And graphically, like this:</p><p><img src=/img/histogram_example.ba5a.png alt="Histogram Example"></p><p>Choosing bins isn’t always straightforward, but you should try to choose them so that they’re not misleading. For instance, how big should each bin be? Your choice can make a big difference in the visualization. Most of the time, bins should be equal in size, but there are some circumstances when that’s not optimal. See <a href="http://blog.amplitude.com/2014/08/06/optimal-streaming-histograms/">this post</a> for an interesting take on that problem.</p><h4 id=probability-mass-function>Probability Mass Function</h4><p>A <a href="http://books.google.com/books?id=ZfRyBS1WbAQC&amp;pg=PT105#v=onepage&amp;q&amp;f=false">Probability Mass Function (PMF)</a> is similar to a histogram, but instead of using frequencies on the y-axis, probabilities are used. Therefore, a PMF can derived by normalizing a data set’s histogram. Note: This normalization step is a must when you’re comparing two histograms of different sample sizes.</p><p>For the same data set we used earlier, here is the PMF:</p><p><img src=/img/pmf_example.6748.png alt="Probability Mass Function Example"></p><h4 id=probability-density-function>Probability Density Function</h4><p>A <a href=http://mathworld.wolfram.com/ProbabilityFunction.html>Probability Density Function (PDF)</a> is analogous to a PMF, but it corresponds to a continuous distribution (whereas our previous examples have been discrete). It’s called a density function because, since there are an infinite number of outcomes, the probability at any single point is 0, so it really only makes sense to talk about the probability of a value range. We will delve further into PDFs in future posts when we look at examples of continuous distributions.</p><h4 id=cumulative-distribution-function>Cumulative Distribution Function</h4><p>A <a href=http://en.wikipedia.org/wiki/Cumulative_distribution_function>Cumulative Distribution Function (CDF)</a> represents the probability that a random variable, X, will be equal <em>or less to</em> a particular value for a given distribution.</p><p>For a discrete distribution, it’s defined as</p><p>\[P(X \le x) = \sum_{x_i \le x} p(x_i) \]</p><p>and for a contintuous distribution, it’s defined as</p><p>\[P(X \le x) = \int_{-\infty}^x f_X(t) dt \text{, where } f_X \text{ is the distribution’s PDF.} \]</p><p>The CDF for our previous example can be calculated with numpy’s cumsum function, and it looks like this:</p><p><img src=/img/cdf_example.6dc9.png alt="Cumulative Distribution Function Example"></p><div class=well><a href="/ipython-notebooks/Probability Terms.ipynb">Download</a> the IPython Notebook that created the preceding graphs.</div><h3 id=descriptive-statistics>Descriptive Statistics</h3><p>Descriptive Statistics are a way of characterizing a distribution by reducing them down to some set of parameters. These terms are extremely common in statistics, so chances are you’re familiar with many of them, but we can get a deeper sense of them by expressing them all in terms of the data representations we just discussed.</p><h4 id=mean>Mean</h4><p>We’re all familiar with calculating the mean of a data set (sum over length), but using PMFs and PDFs, we can get at a much more precise definition.</p><p>Let \(P(x)\) be defined as a PMF for a discrete distribution over the values \(x_1, x_2, …\), then for a random variable, \(X\), the expected value will be</p><p>\[E(X) = \sum_{\forall i} x_i \ P(x_i) = \mu\]</p><p>Likewise, for a continuous distribution with \(P(x)\) as its PDF,</p><p>\[E(X) = \int_{-\infty}^\infty x \ P(x) \ dx = \mu\]</p><p>These definitions are more general than simply taking the arithmetic mean of a data set. In fact, when working with a raw data set, dividing the sum by the length is actually using the above formula for discrete distributions, where each data point has a probability of \(\frac{1}{n}\). However, if you don’t have the raw data for a distribution, but you have a function to define its PMF (or PDF), then you can still use these functions to determine the mean.</p><h4 id=mode>Mode</h4><p>The most frequent value in the data. In a PMF or PDF, it will correspond to this highest point in the graph.</p><h4 id=percentile>Percentile</h4><p>For a value \(x\), its percentile rank is the percent of values that are less than or equal to \(x\). From a CDF, it’s easy to glean percentile ranks. For the \(n\)th percentile, look for where the point at which the CDF crosses \(\frac{n}{100}\) on the y-axis.</p><h4 id=median>Median</h4><p>While there are other definitions for median out there, the one we’ll be using is the 50th percentil rank. So, on a CDF, it’s where 0.5 is crossed on the y-axis. Using this definition, the area under the curve of the PMF (or PDF) to the left of the median and to the right are equal. In these terms, then, probability mass is analogous to the center of mass.</p><h4 id=central-moment>Central Moment</h4><p>Thinking of the median as the center of probability mass makes an interesting connection to the physical world, and we can take that connection further. Similar to <a href=http://en.wikipedia.org/wiki/Moment_(physics)>moments in physics</a>, a <a href=http://en.wikipedia.org/wiki/Moment_(mathematics)>moment in mathematics</a> describes the distribution of mass (in this case, probability mass). It will be beneficial to talk about moments centered around the mean, called <a href=http://www.cs.gmu.edu/~menasce/cs700/files/ProbabilityandDiscreteProbabilityDistributions.pdf>central moments</a>.</p><p>The \(k\)th central moment is defined as</p><p>\[E[(X - \mu)^k] = \sum_{\forall i} (X_i - \mu)^k \times P(X_i)\]</p><p>The 0th central moment is just the sum total of the mass, and since we’re talking probabilities here, that is always going to be 1.</p><p>The 1st central moment is always 0 because it calculates where the mean is in relation to the mean, somewhat tautologically.</p><p>So far, central moments seem like dull descriptors, but things start getting interesting with the 2nd central moment.</p><h4 id=variance-and-standard-deviation>Variance and Standard Deviation</h4><p>The <a href=http://en.wikipedia.org/wiki/Variance>variance</a> of a data set describes how spread out the data points are from each other. It’s always positive. A variance of 0 represents a data set where all data points are identical, and the greater the variance, the more the spread.</p><p>Variance is the second central moment, which is defined as</p><p>\[Var(X) = \sigma^2 = \sum_{\forall i} (X_i - \mu)^2 \times P(X_i)\]</p><p>Since variance is not in the same units as the mean and the data itself, standard deviation is often used instead, which is just the square root of variance.</p><h4 id=skewness>Skewness</h4><p>Though the exact relationship is hard to pin down, <a href=http://en.wikipedia.org/wiki/Skewness>skewness</a> is roughly a measure of distribution symmetery, with a perfectly symmetric distribution having a skew value of 0. It represents the relationship between the left tail and the right tail of the data. The tails of the data are determined by how far data points stretch out below or above the central tendency of the data. If the left tail is longer, the skew will be negative, and if the right tail is longer, the skew will be positive.</p><p><img src=/img/skewness_example.7eb0.png alt="Right Skewness Versus Left Skewness"></p><p>Skewness is defined in terms of the third central moment as</p><p>\[Skew(X) = \frac{\mu_3}{\sigma^3}\]</p><p>where \(\mu_3\) is the third central moment and \(\sigma\) is standard deviation.</p><h4 id=kurtosis>Kurtosis</h4><p>The least well known of the descriptive statistics we’ll be discussing (but unarguably the one with the coolest name) is <a href=http://en.wikipedia.org/wiki/Kurtosis>kurtosis</a>. It is a measure of “peakedness” or of the tails’ weights.</p><p>This image from <a href=http://www.columbia.edu/~ld208/psymeth97.pdf>a paper</a> by Lawrence T. DeCarlo illustrates a distribution with a positive kurtosis on the left and a negative kurtosis on the right (as compared with a normal distribution, which has a kurtosis of 0 and is in both figures as a dotted line).</p><p><img src=/img/kurtosis_example.570b.png alt="Positive Kurtosis Versus Negative Kurtosis"></p><p>Kurtosis has multiple definitions, but the most common (sometimes called excess kurtosis) is defined as</p><p>\[Kurt(X) = \frac{\mu_4}{\sigma^4} - 3\]</p><p>where \(\mu_4\) is the fourth central moment and \(\sigma\) is again the standard deviation.</p><p>The minus 3 might look a little arbitrary, but it’s to give a normal distribution a kurtosis of 0.</p><h4 id=beyond>Beyond</h4><p>There’s nothing stopping us from continuing down this path of central moments ad infinitum, but I think we’ve already explored enough. One interesting thing to note, though, is that “[f]or a bounded distribution of mass or probability, the collection of all the moments (of all orders, from 0 to ∞) uniquely determines the distribution.” (<a href=http://en.wikipedia.org/wiki/Moment_(mathematics)>From Wikipedia</a>)</p><p>All of the descriptors we’ve covered can be very useful, depending on the circumstance. For instance, if you know you have a normal distribution, all you need is the mean and the standard deviation to define the distribution. If you’re not sure if you have a normal distribution, you can calculate the skewness and excess kurtosis. If both of these values are close to 0, then you probably have a normal distribution.</p><p>We will definitely be using all of these data representations and descriptive statistics in future posts, and any credible data scientist should have a firm grasp of them.</p></div><div class=comments><div id=disqus_thread></div><script type=text/javascript>/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
	        var disqus_shortname = 'datajourneyman'; // required: replace example with your forum shortname

	        /* * * DON'T EDIT BELOW THIS LINE * * */
	        (function() {
	            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	        })();</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=http://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><script src=//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js></script><script src=//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js></script><script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({
		"HTML-CSS": { linebreaks: { automatic: true } },
		SVG: { linebreaks: { automatic: true } },
		TeX: { 
            Macros: { 
              goodbreak: '\\mmlToken{mo}[linebreak="goodbreak"]{}', 
              badbreak: ['\\mmlToken{mo}[linebreak="badbreak"]{#1}',1], 
              nobreak: ['\\mmlToken{mo}[linebreak="nobreak"]{#1}',1], 
              invisibletimes: ['\\mmlToken{mo}{\u2062}'] 
            } 
        } 
    });</script><script src=/js/scripts.08ae.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-10466691-5', 'auto');
  ga('send', 'pageview');</script></body></html>